\capitulo{3}{Conceptos teóricos}

% En aquellos proyectos que necesiten para su comprensión y desarrollo de unos conceptos teóricos de una determinada materia o de un determinado dominio de conocimiento, debe existir un apartado que sintetice dichos conceptos.

\section{Minería de datos}\label{DefMineria}

Es el proceso mediante el cual se pretende extraer conocimiento de un conjunto de datos que, sin ser tratados o analizados previamente, no nos proporcionan información útil \cite{DataMiningConcepts}.

Se trata de un término que podría generar confusión con el de KDD (\textit{Knowledge Discovery from Data} \cite{fayyad1996data}), siendo en ocasiones tratado como un mero sinónimo de este término (que apareció antes que el de minería de datos) y en otras siendo descrito como un mero proceso dentro del descubrimiento de información, encargado de obtener conocimiento mediante la aplicación de algoritmos sobre datos recibidos~\cite{DataMiningConcepts}. 

A lo largo de esta memoria trataremos a la minería de datos como sinónimo de KDD, esto es, el conjunto de procesos que comprenden desde el pre procesamiento de los datos hasta la obtención y presentación de información la información útil que contienen.


\section{Algoritmos de selección de instancias}\label{sec:DefAlgSel}

El objetivo de estos algoritmos es solucionar dos problemas que afectan a la minería de datos: la cantidad cada vez mayor de datos, y su calidad. Podremos, por lo tanto,  definirlos como una herramienta para extraer, de un conjunto de instancias, aquellas que conocemos, o sospechamos, son superfluas o perjudiciales~\cite{IntroInstanceSelect}.

Eliminando una porción del conjunto de instancias durante la fase de pre procesamiento de los datos conseguimos que el tiempo de ejecución algoritmos posteriores se reduzca, dado que hay menos instancias a examinar, mientras que es posible mejorar los resultados obtenidos al finalizar el proceso de minería~\cite{IntroInstanceSelect}.

\begin{comment}
Podemos clasificar estos algoritmos de acuerdo al tipo de instancias que eliminemos ~\cite{Garcia2012}. Así pues, podemos distinguir entre:

\begin{itemize}
	\item \textbf{Algoritmos de condensación:} Se concentra en mantener las instancias cercanas a las fronteras de decisión entre las clases.
	\item \textbf{Algoritmos de edición:} Eliminan instancias ruidosas así como aquellas que se encuentran muy cerca de la frontera de decisión, intentando mejorar la calidad del conjunto y produciendo una frontera entre clases mucho más suave.
	\item \textbf{Algoritmos híbridos:} Intentan encontrar el subconjunto de instancias más pequeño posible que mantenga o incluso mejore el resultado que más tarde conseguirá el algoritmo de minería de datos que apliquemos. Para ello permite eliminar cualquier tipo de instancias. 
\end{itemize}

De la misma manera, según la forma en la que se construya el nuevo conjunto de datos podemos diferenciar entre las siguientes clases de algoritmos~\cite{Garcia2011}:

\begin{itemize}
	\item \textbf{De incremento:} Partimos de un conjunto vacío o con unas pocas instancias significativas y añadimos nuevas instancias con cada iteración del algoritmo.
	\item \textbf{De decremento:} Comenzamos con todas las instancias y mediante operaciones de eliminación conseguimos un nuevo conjunto de tamaño menor.
	\item \textbf{Fijos:} Desde el principio el usuario indica el número final de instancias al que queremos reducir un conjunto. Son algoritmos muy dependientes del conjunto de datos utilizado. 
	\item \textbf{Mixto:} Comenzamos con un subconjunto de instancias, obtenidas de manera aleatoria o tras aplicar otro algoritmo de selección, y realizamos operaciones para añadir o eliminar instancias de ese dicho conjunto.
\end{itemize}

\end{comment}

\subsection{Locality sensitive hashing instance selection (LSHIS)}\label{sec:defLSHIS}

LSH, no confundir con el algoritmo de selección de instancias que definiremos a continuación, es un algoritmo que permite identificar y agrupar elementos muy semejantes. Su comportamiento se basa en un conjunto de funciones \textit{hash} que tienen la característica fundamental la capacidad de asignar elementos similares a un mismo grupo (\textit{bucket}) con una alta probabilidad~\cite{LSHISPaper}.

El algoritmo LSHIS es un método de selección de instancias que se apoya en el uso de LSH. La idea es aplicar, sobre las instancias iniciales, un conjunto de funciones hash que permitan agrupar en un mismo bucket aquellas instancias con un alto grado de similitud. Posteriormente, y realizando este proceso durante varias iteraciones si es preciso, de cada uno de esos buckets seleccionaremos una instancia de cada clase para formar el conjunto de instancias final. 

La ventaja de este algoritmo frente a otras alternativas es que permite realizar la selección de instancias en un tiempo de complejidad lineal, en comparación con soluciones de complejidad cuadrática o logarítmica~\cite{LSHISPaper}.


%LSHIS-PSEUDOCÓDIGO DEL PAPER
\begin{algorithm*}
\DontPrintSemicolon
\KwIn{Conjunto de instancias $ X = \lbrace(\mathbf{x}_{1},y_{1}),...,(\mathbf{x}_{n},y_{n})\rbrace$,
      conjunto $\mathcal{G}$ de familias de funciones hash}
\KwOut{Conjunto de instancias seleccionado $ S \subset X $ }

$ S = \varnothing $

\ForEach {instancia $ \mathbf{x} \in X $} {
  \ForEach {familia de funciones $g \in\mathcal{G} $} {
     $u\leftarrow$ cubeta asignada por la familia $g$ a la instancia $\mathbf{x}$ \;
     \If {no existen otras instancias de la misma clase que $\mathbf{x}$ en $u$}
     {
       Añadir $ \mathbf{x} $ a $ S $ \;
       Añadir $ \mathbf{x} $ a $ u $ \;
     }
  }
}

\Return {$S$}
\caption{LSH-IS -- Algoritmo de selección de instancias mediante hashing. \cite{LSHISPaper}}
\label{alg:LSHIS}
\end{algorithm*}

\subsection{Democratic Instance Selection}\label{sec:defDemoIS}

El algoritmo DemoIS es un método de selección de instancias que consiste en aplicar, durante un número variable de rondas, algoritmos de selección de instancias sobre subconjuntos disjuntos del gran conjunto inicial. En cada iteración, este algoritmo asignará unos ``votos'' a las instancias dependiendo de si han sido o no seleccionadas. Concluidas las rondas de votación, se realizará un cálculo con una función de \textit{fitness} para seleccionar todas aquellas instancias cuyos votos no hayan superado un determinado límite \cite{DemoISPaper}.
 
Al igual que el algoritmo LSHIS mencionado anteriormente, la motivación fundamental es la de crear un algoritmo que requiera una carga computacional menor que las soluciones tradicionales. En este caso, eso se consigue mediante la división del conjunto de datos original en otros más pequeños y, por supuesto, el correcto análisis de los resultados anteriores.


%LSHIS-PSEUDOCÓDIGO DEL PAPER
\begin{algorithm*}
\DontPrintSemicolon
\KwIn{Conjunto de instancias $ T = \lbrace(\mathbf{x}_{1},y_{1}),...,(\mathbf{x}_{n},y_{n})\rbrace$,
      tamaño de los subconjuntos $s$,
      número de rondas $r$}
\KwOut{Conjunto de instancias seleccionado $ S \subset T $ }

\For {$k = 1$ to $r$}{

  Dividimos las instancias en subjuntos disjuntos $t_j$ de tamaño $s$ tal que $\bigcup_i t_j = T$
  
  \For {$j = 1$ to $s$} {
  
  	Aplicamos un algoritmo de selección de instancias a $t_j$
  	
  	Añadimos un voto a las instancias removidas de $t_j$
  	
  }

}
Calculamos la mejor función fitness en base a los votos

$v$ = Votos que producen la mejor función fitness

$S = T$

Eliminamos de S todas las instancias cuyo número de votos sea $\geq v$

\Return {$S$}
\caption{LSH-IS -- Algoritmo de selección de instancias Democratic instance selection. \cite{DemoISPaper}}
\label{alg:DemoIS}
\end{algorithm*}




\section{Computación paralela}\label{sec:CompParalela}

La computación paralela es un tipo de computación en la que múltiples operaciones son llevadas a cabo simultáneamente \cite{Almasi:1989}. Parte del principio de que algunos problemas pueden subdividirse en problemas independientes más pequeños que pueden resolverse al mismo tiempo.

Es un paradigma que desde el principio se usó para operaciones que requiriesen una gran carga computacional, pero que ha despertado mucho interés en los últimos años gracias a la fácil escalabilidad (ver \ref{sec:DefEscalabilidad}) que puede ofrecer frente a otras alternativas, como el aumento de la frecuencia de los procesadores, que han alcanzado un punto donde resulta más difícil avanzar \cite{CompParalelaWiki}.

En lo que se refiere al uso de la memoria por parte de un sistema paralelo, existe la posibilidad de que la memoria sea compartida o distribuida dependiendo de si las unidades de procesamiento poseen un espacio de memoria común o cada unidad posee su propio espacio. En lo que se refiere a la tecnología que vamos a usar durante la realización del proyecto (ver \ref{sec:DefSpark}), la memoria siempre será distribuida \cite{SparkPaper}.

\section{Escalabilidad}\label{sec:DefEscalabilidad}

Escalabilidad es la capacidad de un sistema para adaptarse y soportar una carga de trabajo cada vez mayor~\cite{Bondi:2000}.

En función de la manera de adaptarse a la nueva carga de trabajo, podemos diferenciar entre la escalabilidad horizontal, cuando añadimos más nodos a nuestro sistema, y vertical, cuando mejoramos las prestaciones de los elementos ya existentes~\cite{EscalabilidadWiki}. Durante el desarrollo de la memoria siempre usaremos el término de escalabilidad para referirnos a la escalabilidad horizontal, pues estaremos hablando de sistemas distribuidos.

También es una característica que puede aplicarse a los algoritmos, para los cuales definimos escalabilidad como la capacidad de funcionar eficientemente cuando se aplican sobre situaciones que requieran una gran carga de trabajo. En lo referente al proyecto, esa situación será, por lo general, tratar con grandes conjuntos de datos.




